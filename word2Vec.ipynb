{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-11 18:43:07.324660: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "import pandas as pd\n",
    "from top2vec import Top2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "total_df = pd.read_csv('./Data/User_Videos_joined.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>uid</th>\n",
       "      <th>text_concat</th>\n",
       "      <th>start_max</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1QAqxiO8VHM</td>\n",
       "      <td>DtdDe6l482ONxFGKUPIZfJs4U4P2</td>\n",
       "      <td>Take the graph of y = sin(x) and transform \\ni...</td>\n",
       "      <td>1394.520</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lg5wznn3IBE</td>\n",
       "      <td>DtdDe6l482ONxFGKUPIZfJs4U4P2</td>\n",
       "      <td>this is the prestigious Walnut cup and,if you ...</td>\n",
       "      <td>1387.679</td>\n",
       "      <td>632.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yTe-haq1ZUc</td>\n",
       "      <td>DtdDe6l482ONxFGKUPIZfJs4U4P2</td>\n",
       "      <td>elon musk recently gave a warning to,mark zuck...</td>\n",
       "      <td>513.360</td>\n",
       "      <td>239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tBQY1MC7Pf8</td>\n",
       "      <td>DtdDe6l482ONxFGKUPIZfJs4U4P2</td>\n",
       "      <td>Russia continues to threaten Elon Musk,but thi...</td>\n",
       "      <td>589.200</td>\n",
       "      <td>285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HxAoQSkMHcw</td>\n",
       "      <td>DtdDe6l482ONxFGKUPIZfJs4U4P2</td>\n",
       "      <td>and so where specifically will you be in,terms...</td>\n",
       "      <td>918.000</td>\n",
       "      <td>334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>7ns4J-DgVyA</td>\n",
       "      <td>UdZxcsQWpDO5BLH4rNerJl9Y7Fz1</td>\n",
       "      <td>so I'm sure a lot of you are absolutely,shocke...</td>\n",
       "      <td>1337.340</td>\n",
       "      <td>565.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>Hue46YwRvSc</td>\n",
       "      <td>UdZxcsQWpDO5BLH4rNerJl9Y7Fz1</td>\n",
       "      <td>A few people in the comments sections of my\\nr...</td>\n",
       "      <td>1035.032</td>\n",
       "      <td>213.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>wSLoSUGbGaA</td>\n",
       "      <td>UdZxcsQWpDO5BLH4rNerJl9Y7Fz1</td>\n",
       "      <td>foreign,[Music],makes I mean these are the pri...</td>\n",
       "      <td>2722.079</td>\n",
       "      <td>1251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>e0plyXEc1Ak</td>\n",
       "      <td>UdZxcsQWpDO5BLH4rNerJl9Y7Fz1</td>\n",
       "      <td>The governing council\\n decided the following:...</td>\n",
       "      <td>4641.570</td>\n",
       "      <td>1290.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>FkS9EdJ2I3U</td>\n",
       "      <td>UdZxcsQWpDO5BLH4rNerJl9Y7Fz1</td>\n",
       "      <td>So I spent the last year with\\nthese keyboards...</td>\n",
       "      <td>863.571</td>\n",
       "      <td>241.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1293 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                           uid  \\\n",
       "0     1QAqxiO8VHM  DtdDe6l482ONxFGKUPIZfJs4U4P2   \n",
       "1     lg5wznn3IBE  DtdDe6l482ONxFGKUPIZfJs4U4P2   \n",
       "2     yTe-haq1ZUc  DtdDe6l482ONxFGKUPIZfJs4U4P2   \n",
       "3     tBQY1MC7Pf8  DtdDe6l482ONxFGKUPIZfJs4U4P2   \n",
       "4     HxAoQSkMHcw  DtdDe6l482ONxFGKUPIZfJs4U4P2   \n",
       "...           ...                           ...   \n",
       "1288  7ns4J-DgVyA  UdZxcsQWpDO5BLH4rNerJl9Y7Fz1   \n",
       "1289  Hue46YwRvSc  UdZxcsQWpDO5BLH4rNerJl9Y7Fz1   \n",
       "1290  wSLoSUGbGaA  UdZxcsQWpDO5BLH4rNerJl9Y7Fz1   \n",
       "1291  e0plyXEc1Ak  UdZxcsQWpDO5BLH4rNerJl9Y7Fz1   \n",
       "1292  FkS9EdJ2I3U  UdZxcsQWpDO5BLH4rNerJl9Y7Fz1   \n",
       "\n",
       "                                            text_concat  start_max   count  \n",
       "0     Take the graph of y = sin(x) and transform \\ni...   1394.520   256.0  \n",
       "1     this is the prestigious Walnut cup and,if you ...   1387.679   632.0  \n",
       "2     elon musk recently gave a warning to,mark zuck...    513.360   239.0  \n",
       "3     Russia continues to threaten Elon Musk,but thi...    589.200   285.0  \n",
       "4     and so where specifically will you be in,terms...    918.000   334.0  \n",
       "...                                                 ...        ...     ...  \n",
       "1288  so I'm sure a lot of you are absolutely,shocke...   1337.340   565.0  \n",
       "1289  A few people in the comments sections of my\\nr...   1035.032   213.0  \n",
       "1290  foreign,[Music],makes I mean these are the pri...   2722.079  1251.0  \n",
       "1291  The governing council\\n decided the following:...   4641.570  1290.0  \n",
       "1292  So I spent the last year with\\nthese keyboards...    863.571   241.0  \n",
       "\n",
       "[1293 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "## this data frame holds the video transcript for each video id for each user with the total number of seconds\n",
    "total_df\n",
    "## text_concat is the transcript, start_max is the total number of seconds for each video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df[total_df.text_concat.isna() == False]\n",
    "total_df = total_df.drop_duplicates()\n",
    "total_df = total_df[total_df.text_concat.isna() == False]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store topic terms, topic info, and video data\n",
    "topic_terms = []\n",
    "topic_info_ls = []\n",
    "topic_vds_ls = []\n",
    "\n",
    "# Loop over each unique user ID\n",
    "for uid in total_df['uid'].unique():\n",
    "    # Filter dataframe for current user ID\n",
    "    df = total_df[total_df['uid'] == uid]\n",
    "    \n",
    "    # Train Top2Vec model on the text data\n",
    "    model = Top2Vec(list(df['text_concat']), embedding_model='universal-sentence-encoder-multilingual')\n",
    "    \n",
    "    # Get topics from the model\n",
    "    topics = model.get_topics()\n",
    "    \n",
    "    # Initialize list to store most separating words\n",
    "    most_seperating_words = []\n",
    "    \n",
    "    # Loop over each topic\n",
    "    for i in range(len(topics[0])):\n",
    "        # Get terms and weights for current topic\n",
    "        terms = topics[0][i]\n",
    "        weights = topics[1][i]\n",
    "        \n",
    "        # Loop over each term\n",
    "        for k in range(len(terms)):\n",
    "            # Add first term to most separating words\n",
    "            if k == 0:\n",
    "                most_seperating_words.append(terms[k])\n",
    "            \n",
    "            # Add term info to topic terms\n",
    "            topic_terms.append({\n",
    "            'cluster':i,\n",
    "            'term': terms[k],\n",
    "            'weight': weights[k],\n",
    "                'uid':uid\n",
    "        })\n",
    "    \n",
    "    # Calculate cosine similarity between word vectors and topic vectors\n",
    "    # the result is a matrix of size (number of words) x (number of topics) to know each word how close it is to each topic\n",
    "    topic_word = cosine_similarity(model.word_vectors, model.topic_vectors)\n",
    "    \n",
    "    # Perform PCA for dimensionality reduction\n",
    "    # this will compress the data into 2 dimensions to be able to plot the topics \n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(topic_word)\n",
    "    \n",
    "    # Initialize list to store PCA results\n",
    "    ls = []\n",
    "    \n",
    "    # Loop over each topic\n",
    "    for i in range(len(topics[0])):\n",
    "        # Get PCA components for current topic\n",
    "        x = pca.components_[0][i]\n",
    "        y = pca.components_[1][i]\n",
    "        \n",
    "        # Add PCA results to list\n",
    "        ls.append({\n",
    "        'cluster' : i,\n",
    "        'x' : x,\n",
    "        'y' : y,\n",
    "            'uid' : uid\n",
    "    })\n",
    "    \n",
    "    # Convert list to dataframe\n",
    "    topic_info_df = pd.DataFrame(ls)\n",
    "    \n",
    "    # Assign topic (cluster) to each document\n",
    "    df['cluster'] = model.doc_top\n",
    "    \n",
    "    # Create new dataframe with video info\n",
    "    topic_videos_df = df[['video_id','start_max','cluster','uid']]\n",
    "    \n",
    "    # Initialize dictionary to store total minutes for each cluster\n",
    "    topic_minutes = {}\n",
    "    \n",
    "    # Calculate total minutes for each cluster\n",
    "    for i in topic_videos_df.groupby('cluster').agg({\n",
    "    'start_max':'sum'\n",
    "    }).iterrows():\n",
    "        topic_minutes[i[0]] = i[1][0] / 60\n",
    "    \n",
    "    # Add total minutes to dataframe\n",
    "    topic_info_df['total_minutes'] = topic_info_df['cluster'].apply(lambda r: topic_minutes[r])\n",
    "    \n",
    "    # Add most separating word to dataframe\n",
    "    topic_info_df['most_seperating_word'] = most_seperating_words\n",
    "    \n",
    "    # Add dataframe info to topic info list\n",
    "    for tp in topic_info_df.iterrows():\n",
    "        topic_info_ls.append(tp[1].to_dict())\n",
    "    \n",
    "    # Add video info to video data list\n",
    "    for vd in topic_videos_df.iterrows():\n",
    "        topic_vds_ls.append(vd[1].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Videos_With_Cluster_df = pd.DataFrame(topic_vds_ls)\n",
    "Topic_Info_df = pd.DataFrame(topic_info_ls)\n",
    "topic_terms_df = pd.DataFrame(topic_terms)\n",
    "                              \n",
    "# Save dataframes to CSV files\n",
    "Topic_Info_df.to_csv('topic_info.csv', index=False)\n",
    "Videos_With_Cluster_df.to_csv('videos_with_cluster.csv', index=False)\n",
    "topic_terms_df.to_csv('topic_terms.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
